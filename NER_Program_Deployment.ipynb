{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags"
      ],
      "metadata": {
        "id": "Pl2VhynX9N1l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBeBdo-C9R-E",
        "outputId": "04f27e72-2d8f-470d-ae58-82832c5efb17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = '706c3d2594104d618b00448e2ebec994'\n",
        "url = f'https://newsapi.org/v2/top-headlines?country=us&apiKey={API_KEY}'\n",
        "\n",
        "response = requests.get(url)\n",
        "data = response.json()\n",
        "\n",
        "if 'articles' not in data or len(data['articles']) == 0:\n",
        "    raise ValueError(\"No articles found in the response.\")\n",
        "\n",
        "article = data['articles'][0]\n",
        "article_content = article.get('content') or article.get('description') or article.get('title')\n",
        "\n",
        "if not article_content:\n",
        "    raise ValueError(\"No article content found or the content is empty. Article data: \" + str(article))\n",
        "\n",
        "print(\"News Article Content:\")\n",
        "print(article_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBNiMzjn9WoA",
        "outputId": "62c1fa49-6b58-40a9-95cf-6858510563fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News Article Content:\n",
            "Iran Arrests Dozens in Search for Suspects in Killing of Hamas Leader - The New York Times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nltk_ner(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    tree = ne_chunk(pos_tags)\n",
        "    iob_tagged = tree2conlltags(tree)\n",
        "    entities = set()\n",
        "    for word, pos, ner in iob_tagged:\n",
        "        if ner != 'O':\n",
        "            entities.add((word, ner))\n",
        "    return entities\n",
        "\n",
        "nltk_entities = nltk_ner(article_content)\n",
        "print(\"\\nEntities extracted using NLTK (Rule-based):\")\n",
        "print(nltk_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI8UXmtO9lOP",
        "outputId": "62a9cf71-2a29-4bee-df6b-7a1ae81dfa3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities extracted using NLTK (Rule-based):\n",
            "{('Suspects', 'B-ORGANIZATION'), ('Times', 'I-GPE'), ('York', 'I-GPE'), ('Iran', 'B-GPE'), ('New', 'B-GPE'), ('Dozens', 'B-GPE'), ('Leader', 'I-ORGANIZATION'), ('Search', 'B-GPE'), ('Hamas', 'B-ORGANIZATION')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ML-based NER using SpaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(article_content)\n",
        "\n",
        "spacy_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"\\nEntities extracted using SpaCy (ML-based):\")\n",
        "print(spacy_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwspaT4p9p10",
        "outputId": "df08ae5c-7a9f-477f-d7dd-065bf0842ea1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entities extracted using SpaCy (ML-based):\n",
            "[('Iran', 'GPE'), ('Dozens', 'CARDINAL'), ('Search for Suspects', 'ORG'), ('Killing', 'GPE'), ('Hamas', 'ORG'), ('The New York Times', 'ORG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_entity_set = set(nltk_entities)\n",
        "spacy_entity_set = set(spacy_entities)\n",
        "\n",
        "common_entities = nltk_entity_set.intersection(spacy_entity_set)\n",
        "nltk_unique_entities = nltk_entity_set - spacy_entity_set\n",
        "spacy_unique_entities = spacy_entity_set - nltk_entity_set\n",
        "\n",
        "print(\"\\nCommon Entities:\")\n",
        "print(common_entities)\n",
        "print(\"\\nUnique Entities to NLTK:\")\n",
        "print(nltk_unique_entities)\n",
        "print(\"\\nUnique Entities to SpaCy:\")\n",
        "print(spacy_unique_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5z35tT49uBV",
        "outputId": "c060c353-fa92-4ae3-aec4-c92c496c1809"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Common Entities:\n",
            "set()\n",
            "\n",
            "Unique Entities to NLTK:\n",
            "{('Suspects', 'B-ORGANIZATION'), ('York', 'I-GPE'), ('Iran', 'B-GPE'), ('Hamas', 'B-ORGANIZATION'), ('New', 'B-GPE'), ('Dozens', 'B-GPE'), ('Leader', 'I-ORGANIZATION'), ('Search', 'B-GPE'), ('Times', 'I-GPE')}\n",
            "\n",
            "Unique Entities to SpaCy:\n",
            "{('Search for Suspects', 'ORG'), ('Iran', 'GPE'), ('Killing', 'GPE'), ('Dozens', 'CARDINAL'), ('The New York Times', 'ORG'), ('Hamas', 'ORG')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDiscussion:\")\n",
        "print(\"The NLTK rule-based approach, using chunking and regular expressions, may not be as accurate as the SpaCy ML-based approach.\")\n",
        "print(\"SpaCy's model is trained on large datasets and can recognize a wider variety of entities and contexts.\")\n",
        "print(\"NLTK might miss some entities or incorrectly classify them, especially in more complex sentences.\")\n",
        "print(\"However, the rule-based approach can be useful in simpler contexts or where model-based approaches are not feasible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWdqKRpT9xYw",
        "outputId": "a1e7da5c-1857-47d3-a6ce-6239369be311"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Discussion:\n",
            "The NLTK rule-based approach, using chunking and regular expressions, may not be as accurate as the SpaCy ML-based approach.\n",
            "SpaCy's model is trained on large datasets and can recognize a wider variety of entities and contexts.\n",
            "NLTK might miss some entities or incorrectly classify them, especially in more complex sentences.\n",
            "However, the rule-based approach can be useful in simpler contexts or where model-based approaches are not feasible.\n"
          ]
        }
      ]
    }
  ]
}